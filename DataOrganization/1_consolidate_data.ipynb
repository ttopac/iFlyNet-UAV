{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(os.path.dirname(cur_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dates = [\"08_01_2022\",\"08_02_2022\"]\n",
    "# test_folders = [\"Day1_Training1\", \"Day2_Training1\"]\n",
    "# file_names = {\"eds\":[\"flagged_DateTimed_WTRUN2_day1_training1_EDS\", \"DateTimed_WTRUN2_day2_training1_EDS\"], \n",
    "#               \"uav\":[\"WTRUN2_day1_training1_UAV\", \"WTRUN2_day2_training1_UAV\"],\n",
    "#               \"rtdstr\":[\"compensated_normalized_WTRUN2_training_sweep1_2022-08-01_17-24-43-50_rtd-str_Jan2023_tworegion\",\n",
    "#                         \"compensated_normalized_WTRUN2_day2_training1_2022-08-02_12-38-30-01_rtd-str_Jan2023_tworegion\"]}\n",
    "\n",
    "test_dates = [\"08_02_2022\"]\n",
    "test_folders = [\"Day2_Dynamic1\"]\n",
    "file_names = {\"eds\":[\"nopeaks_DateTimed_WTRUN2_day2_dynamic1_EDS\"], \n",
    "              \"uav\":[\"WTRUN2_day2_dynamic1_UAV\"],\n",
    "              \"rtdstr\":[\"compensated_normalized_WTRUN2_day2_dynamic1_2022-08-02_14-32-54-11_rtd-str_Jan2023_tworegion\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in the EDS, UAV, and RTD-STR (IMGenie) data as Pandas Dataframes\n",
    "test_dfs = {\"eds\": list(), \"uav\": list(), \"rtdstr\": list()}\n",
    "eds_rtdstr_offsets = list() #each item: [\"vantage stream name\", \"number of datapoints until reaching to vantage from other stream\"]\n",
    "vantage_uav_offsets = list() #each item: \"number of datapoints until reaching to vantage from UAV\"\n",
    "\n",
    "for i in range(len(test_dates)):\n",
    "  data_dir = os.path.join(main_dir, test_dates[i]+\"_Tests\", \"testdata\", test_folders[i])\n",
    "  for sensor_type in (\"eds\", \"uav\", \"rtdstr\"):\n",
    "    test_csv = os.path.join(data_dir, file_names[sensor_type][i]+\".csv\")\n",
    "    test_df = pd.read_csv(test_csv, header=0)\n",
    "    test_dfs[sensor_type].append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert DateTime objects to the dataframes.\n",
    "\n",
    "#Insert for EDS data:\n",
    "for i in range(len(test_dfs[\"eds\"])):\n",
    "  if \"eds_DateTime Obj\" not in test_dfs[\"eds\"][i].columns:\n",
    "    eds_times = test_dfs['eds'][i][\"DateTime Str\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    test_dfs['eds'][i].insert(1, \"eds_DateTime Obj\", eds_times)\n",
    "  else:\n",
    "    test_dfs['eds'][i][\"eds_DateTime Obj\"] = test_dfs['eds'][i][\"eds_DateTime Obj\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "#Insert for RTD-STR data:\n",
    "for i in range(len(test_dfs[\"rtdstr\"])):\n",
    "  if \"rtdstr_DateTime Obj\" not in test_dfs[\"rtdstr\"][i].columns:\n",
    "    month = int (test_dates[i][0:2])\n",
    "    day = int (test_dates[i][3:5])\n",
    "    year = int (test_dates[i][6:])\n",
    "    rtdstr_times = test_dfs['rtdstr'][i][\"Date/Time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d_%H-%M-%S-%f\"))\n",
    "    rtdstr_times = rtdstr_times.apply(lambda x: datetime.datetime(year, month, day, x.hour, x.minute, x.second, x.microsecond)) #We do this because we haven't recorded date in the original data.\n",
    "    test_dfs['rtdstr'][i].insert(1, \"rtdstr_DateTime Obj\", rtdstr_times)\n",
    "  else:\n",
    "    test_dfs['rtdstr'][i][\"rtdstr_DateTime Obj\"] = test_dfs['rtdstr'][i][\"rtdstr_DateTime Obj\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "\n",
    "#Insert for UAV data:\n",
    "#First define a function to separate full seconds into hours, minutes, seconds, and milliseconds\n",
    "def seperate_seconds(seconds):\n",
    "  #This function assumes inserted seconds is shorter than 24 hours = 86,400 seconds\n",
    "  onlyseconds = int(seconds.split(\".\")[0])\n",
    "  hours = int (onlyseconds/(60*60))\n",
    "  minutes = int (onlyseconds/(60)%60)\n",
    "  remseconds = int (onlyseconds - hours*60*60 - minutes*60)\n",
    "  milliseconds = seconds.split(\".\")[1][0:6]\n",
    "  return hours, minutes, remseconds, milliseconds\n",
    "\n",
    "for i in range(len(test_dfs[\"uav\"])):\n",
    "  if \"uav_DateTime Obj\" not in test_dfs[\"uav\"][i].columns:\n",
    "    seconds = test_dfs['uav'][i][\"STATIC_PRESSURE_TIME\"].astype(str)\n",
    "    time_sr = seconds.apply(seperate_seconds)\n",
    "    uav_times = time_sr.astype(str).apply(lambda x: datetime.datetime.strptime(x, \"(%H, %M, %S, '%f')\"))\n",
    "    test_dfs['uav'][i].insert(1, \"uav_DateTime Obj\", uav_times)\n",
    "  else:\n",
    "    test_dfs['uav'][i][\"uav_DateTime Obj\"] = test_dfs['uav'][i][\"uav_DateTime Obj\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the offset between the start points of EDS and IMGenie data\n",
    "for i in range(len(test_dfs[\"eds\"])):\n",
    "  first_time_eds = test_dfs[\"eds\"][i][\"eds_DateTime Obj\"][0] \n",
    "  first_time_rtdstr = test_dfs[\"rtdstr\"][i][\"rtdstr_DateTime Obj\"][0]\n",
    "  vantage_stream = \"eds\" if first_time_eds > first_time_rtdstr else \"rtdstr\" #Vantage stream is the stream that start later.\n",
    "  early_stream = \"eds\" if first_time_eds < first_time_rtdstr else \"rtdstr\" #Early stream is the other one\n",
    "  eds_rtdstr_offsets.append([vantage_stream])\n",
    "\n",
    "  vantage_time = test_dfs[vantage_stream][i][f\"{vantage_stream}_DateTime Obj\"][0] \n",
    "  offset_ix = 1\n",
    "  time_delta = datetime.timedelta(seconds=999) #(assume this is large enough)\n",
    "  while True:\n",
    "    incrementing_time = test_dfs[early_stream][i][f\"{early_stream}_DateTime Obj\"][offset_ix] \n",
    "    new_time_delta = abs(vantage_time - incrementing_time)\n",
    "    if new_time_delta > time_delta:\n",
    "      break\n",
    "    time_delta = new_time_delta\n",
    "    offset_ix += 1\n",
    "  eds_rtdstr_offsets[-1].append(offset_ix)\n",
    "\n",
    "offsets_savedir = os.path.join(cur_dir, 'offset_pickles')\n",
    "with open(os.path.join(offsets_savedir,'eds_rtdstr_offsets_dynamic1.pkl'), 'wb') as f:\n",
    "  pickle.dump(eds_rtdstr_offsets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eds', 281]]\n"
     ]
    }
   ],
   "source": [
    "print(eds_rtdstr_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the offset between the start points of UAV and the vantage stream we found out above.\n",
    "#We assume UAV always starts before vantage because of how we conducted the experiments.\n",
    "observe_column_name = {\"eds\":\"Inclination (deg)\", \"uav\":\"Pitch (deg)\"}\n",
    "for i in range(len(test_dfs[\"uav\"])):\n",
    "  aoajump_ixs = list() #[[uav_ix, eds_ix]]\n",
    "  for data_stream in (\"uav\", \"eds\"):\n",
    "    #Find the indices of eds and uav streams where Inclination (deg) and Pitch (deg) increases by >4 degrees in 5 seconds.\n",
    "    #Calculate the average time delta between each row.\n",
    "    time_delta = (test_dfs[data_stream][i][f\"{data_stream}_DateTime Obj\"][200] - test_dfs[data_stream][i][f\"{data_stream}_DateTime Obj\"][100])/100\n",
    "\n",
    "    #Calculate diff for 5 seconds\n",
    "    num_rows = int(5/time_delta.total_seconds())\n",
    "    angle_change = test_dfs[data_stream][i][observe_column_name[data_stream]].diff(periods=num_rows)\n",
    "\n",
    "    #Extract the row ID where we see the first >4 degree change in 5 seconds (+10 degree calibration procedure).\n",
    "    aoajump_ix = angle_change [angle_change>4].index[0]\n",
    "\n",
    "    #Process the special case when UAV had a weird AoA jump in the Aug 2, 2022 training test.\n",
    "    if i==1 and data_stream == \"uav\":\n",
    "      j = 0\n",
    "      reftime = datetime.datetime(1900, 1, 1, 0, 0, 0)\n",
    "      timestamp = test_dfs[data_stream][i][f\"{data_stream}_DateTime Obj\"][aoajump_ix]\n",
    "      print (\"data_type =\", data_stream)\n",
    "      print (\"i =\",i)\n",
    "      _ = input(\"Warning! Entering into a special case designed for WTRUN2_day2_training1_UAV file. Make sure you're processing this file here.\")\n",
    "      while timestamp-reftime < datetime.timedelta(seconds=1200):\n",
    "        j += 1\n",
    "        aoajump_ix = angle_change [angle_change>4].index[j]\n",
    "        timestamp = test_dfs[data_stream][i][f\"{data_stream}_DateTime Obj\"][aoajump_ix]\n",
    "      \n",
    "    aoajump_ixs.append(aoajump_ix)\n",
    "\n",
    "  eds_aoajump_timestamp = test_dfs[\"eds\"][i][\"eds_DateTime Obj\"][aoajump_ixs[1]]  \n",
    "  if eds_rtdstr_offsets[i][0] == \"eds\": #If vantage is eds\n",
    "    aoajump_delta_w_vantage = eds_aoajump_timestamp - test_dfs[\"eds\"][i][\"eds_DateTime Obj\"][0]\n",
    "  else: #If vantage is rtdstr\n",
    "    aoajump_delta_w_vantage = eds_aoajump_timestamp - test_dfs[\"rtdstr\"][i][\"rtdstr_DateTime Obj\"][0]\n",
    "\n",
    "  uav_aoajump_timestamp = test_dfs[\"uav\"][i][\"uav_DateTime Obj\"][aoajump_ixs[0]]\n",
    "  time_to_find = uav_aoajump_timestamp - aoajump_delta_w_vantage\n",
    "  \n",
    "  time_delta = datetime.timedelta(seconds=999) #(assume this is large enough)\n",
    "  search_ix = 1\n",
    "  timeshift_reps = 0\n",
    "  while True:\n",
    "    new_timedelta = abs (test_dfs[\"uav\"][i][\"uav_DateTime Obj\"][search_ix] - time_to_find)\n",
    "    if new_timedelta > time_delta:\n",
    "      timeshift_reps += 1\n",
    "      if timeshift_reps > 3:\n",
    "        search_ix -= 3\n",
    "        break\n",
    "    else:\n",
    "      timeshift_reps = 0\n",
    "    search_ix += 1\n",
    "    time_delta = new_timedelta\n",
    "  uav_vantage_ix = search_ix\n",
    "  vantage_uav_offsets.append(uav_vantage_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2766]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vantage_uav_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xn/6htkyn1918s2p78bt7v282d00000gn/T/ipykernel_73671/2435574352.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dfs['uav'][i][\"uav_DateTime Obj\"].loc[start_ixs[\"uav\"]:] = test_dfs['uav'][i][\"uav_DateTime Obj\"].loc[start_ixs[\"uav\"]:] - uav_time_offset + rtdstr_time_offset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing uav\n",
      "We are at line = 0\n",
      "We are at line = 100000\n",
      "We are at line = 200000\n",
      "We are at line = 300000\n",
      "We are at line = 400000\n",
      "We are at line = 500000\n",
      "We are at line = 600000\n",
      "We are at line = 700000\n",
      "We are at line = 800000\n",
      "We are at line = 900000\n",
      "Processing eds\n",
      "We are at line = 0\n",
      "We are at line = 100000\n",
      "We are at line = 200000\n",
      "We are at line = 300000\n",
      "We are at line = 400000\n",
      "We are at line = 500000\n",
      "We are at line = 600000\n",
      "We are at line = 700000\n",
      "We are at line = 800000\n",
      "We are at line = 900000\n"
     ]
    }
   ],
   "source": [
    "# Construct new df objects that will contain all RTDSTR, EDS, and UAV data:\n",
    "#   -One df object will contain one experiment run (such as Day1_Training1)\n",
    "#   -First timepoint will match with that of the vantage stream.\n",
    "#   -The timepoints will come from the RTDSTR df. EDS and UAV will interpolate according to:\n",
    "#       -EDS: Iterating row-wise in RTDSTR data, find the closest timematch in EDS data and bring that block to this row.\n",
    "#       -UAV: First, change the DateTime Obj of UAV that matches the vantage with the RTDSTR at the vantage.\n",
    "#             Then, apply that offset to the subsequent rows.\n",
    "#             Then apply above idea to find closest match in UAV data.\n",
    "#   -Last datapoint will be the earlier of the duration of 3 streams. Can be thought of another ending vantage point.\n",
    "#   -Then we'll concatenate the dfs by propagating the time to second and so on.\n",
    "\n",
    "combined_dfs = list()\n",
    "end_loc_offsets = list()\n",
    "for i in range(len(test_folders)):\n",
    "  #Get the first timepoint from vantage and all subsequent timepoints from RTDSTR\n",
    "  start_ixs = dict()\n",
    "  vantage = eds_rtdstr_offsets[i][0]\n",
    "  start_ixs[\"rtdstr\"] = 0 if vantage==\"rtdstr\" else eds_rtdstr_offsets[i][1]-1\n",
    "  start_ixs[\"eds\"] = 0 if vantage==\"eds\" else eds_rtdstr_offsets[i][1]-1\n",
    "  start_ixs[\"uav\"] = vantage_uav_offsets[i]\n",
    "\n",
    "  #Adjust UAVs DateTime Objs:\n",
    "  uav_time_offset = test_dfs['uav'][i][\"uav_DateTime Obj\"][start_ixs[\"uav\"]]\n",
    "  rtdstr_time_offset = test_dfs['rtdstr'][i][\"rtdstr_DateTime Obj\"][start_ixs[\"rtdstr\"]]\n",
    "  test_dfs['uav'][i][\"uav_DateTime Obj\"].loc[start_ixs[\"uav\"]:] = test_dfs['uav'][i][\"uav_DateTime Obj\"].loc[start_ixs[\"uav\"]:] - uav_time_offset + rtdstr_time_offset\n",
    "\n",
    "  #Construct the new DataFrame by first bringing the RTDSTR and then matching datapoints from UAV and EDS:\n",
    "  main_df = test_dfs['rtdstr'][i].iloc[start_ixs[\"rtdstr\"]:].reset_index()\n",
    "  main_df = main_df.rename(columns={'index':\"rtdstr_orig_index\"})\n",
    "  \n",
    "  #Merge in the UAV and EDS data to the newly created main_df by carefully according to the algorithm above.\n",
    "  for stream in ('uav', 'eds'):\n",
    "    print (f\"Processing {stream}\")\n",
    "    streamrow = start_ixs[stream]\n",
    "    streamdf_dat = np.full([main_df.shape[0], test_dfs[stream][0].columns.size], None)\n",
    "    streamdf = pd.DataFrame(streamdf_dat, columns=test_dfs[stream][0].columns)\n",
    "    \n",
    "    #We iterate over the rows of main_df, which is based on the rtdstr data (because it is the densest),\n",
    "    #and finding closest match in streaming data to fuse in.\n",
    "    for maindf_index, row in main_df.iterrows():\n",
    "      while True:\n",
    "        try:\n",
    "          curr_time_stream = test_dfs[stream][i][f\"{stream}_DateTime Obj\"][streamrow]\n",
    "          next_time_stream = test_dfs[stream][i][f\"{stream}_DateTime Obj\"][streamrow+1]\n",
    "          while next_time_stream < curr_time_stream: #If there's problem in time acquisition (accurring sometimes in UAV data)\n",
    "            streamrow += 1\n",
    "            curr_time_stream = test_dfs[stream][i][f\"{stream}_DateTime Obj\"][streamrow]\n",
    "            next_time_stream = test_dfs[stream][i][f\"{stream}_DateTime Obj\"][streamrow+1]\n",
    "          gap_curr = abs(main_df[\"rtdstr_DateTime Obj\"][maindf_index] - curr_time_stream)\n",
    "          gap_next = abs(main_df[\"rtdstr_DateTime Obj\"][maindf_index] - next_time_stream)\n",
    "        except: #To avoid KeyError when we reach the end of test_dfs[stream].\n",
    "          print (\"Exception occurred\")\n",
    "          streamrow += 1\n",
    "          break \n",
    "        if gap_curr < gap_next:\n",
    "          break\n",
    "        streamrow += 1\n",
    "      try:  \n",
    "        streamdf.iloc[maindf_index] = test_dfs[stream][i].iloc[streamrow:streamrow+1]\n",
    "      except:\n",
    "        pass\n",
    "      if maindf_index % 100000 == 0:\n",
    "        print (f\"We are at line = {maindf_index}\")\n",
    "    main_df = main_df.merge(streamdf, how='left', left_index=True, right_index=True)\n",
    "\n",
    "  #Find the last datapoint to cut off the data there.\n",
    "  #First, find out which of the streams end the earliest. That'll determine our cutoff point.\n",
    "  timeref_start = main_df[\"rtdstr_DateTime Obj\"]\n",
    "  shortest_duration = datetime.timedelta(days=1) #1 day should be long enough\n",
    "  for stream in ('rtdstr', 'uav', 'eds'):\n",
    "    stream_duration = test_dfs[stream][i][f\"{stream}_DateTime Obj\"].iloc[-1] - timeref_start[0]\n",
    "    if stream_duration < shortest_duration:\n",
    "      shortest_duration = stream_duration\n",
    "      shortest_stream = stream\n",
    "  timeref_end = test_dfs[shortest_stream][i][f\"{shortest_stream}_DateTime Obj\"].iloc[-1]\n",
    "  \n",
    "  #Then find what timepoint this cutoff will correspond to in our main_df.\n",
    "  #If we find rtdstr above, the \"end_loc\" value we'll get here will be -1 and we'll retain all data.\n",
    "  timedelta = datetime.timedelta(days=1)\n",
    "  backiter_j = -1\n",
    "  while True:\n",
    "    gap = abs(main_df[\"rtdstr_DateTime Obj\"].iloc[backiter_j] - timeref_end)\n",
    "    if gap > timedelta:\n",
    "      break\n",
    "    backiter_j -= 1 #We will come here at least once. That's why we have backiter_j+1 below.\n",
    "    timedelta = gap\n",
    "  end_loc_offset = backiter_j + 1\n",
    "  main_df = main_df.iloc[0:end_loc_offset]\n",
    "  \n",
    "  combined_dfs.append(main_df)\n",
    "  end_loc_offsets.append(end_loc_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the consolidated data files for later reuse\n",
    "df_save_dir = os.path.join(main_dir, \"ConsolidatedData\", \"Dynamic1_Jan2023\")\n",
    "for i in range(len(test_folders)):\n",
    "  combined_dfs[i].to_pickle(os.path.join(df_save_dir,f'nopeaks_consolidated_{i}.pkl'))\n",
    "\n",
    "#Save the number of lines trailed in rtdstr after shortest stream is ended. \n",
    "#If this ends up being -1, it means rtdstr stream ended earliest.\n",
    "with open(os.path.join(offsets_savedir,'rtdstr_offsets_trailing_dynamic1.pkl'), 'wb') as f:\n",
    "  pickle.dump(end_loc_offsets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat the n files from the data into a single consolidated pd.DataFrame\n",
    "consolidated_df = pd.concat((test_df for test_df in combined_dfs), ignore_index=True)\n",
    "consolidated_df.to_pickle(os.path.join(df_save_dir,'nopeaks_consolidated_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cae21afe383f91ea82811178060e59cc3e5895f013d1e4afce35f65192471b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
