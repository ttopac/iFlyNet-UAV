{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(os.path.dirname(cur_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dates = [\"08_01_2022\",\"08_02_2022\"]\n",
    "# test_folders = [\"Day1_Training1\", \"Day2_Training1\"]\n",
    "# rtdstr_filenames = [\"compensated_normalized_WTRUN2_training_sweep1_2022-08-01_17-24-43-50_rtd-str_Jan2023_tworegion\",\n",
    "#                     \"compensated_normalized_WTRUN2_day2_training1_2022-08-02_12-38-30-01_rtd-str_Jan2023_tworegion\"]\n",
    "test_dates = [\"08_02_2022\"]\n",
    "test_folders = [\"Day2_Dynamic1\"]\n",
    "rtdstr_filenames = [\"compensated_normalized_WTRUN2_day2_dynamic1_2022-08-02_14-32-54-11_rtd-str_Jan2023_tworegion\"]\n",
    "\n",
    "eds_rtdstr_offsets_dir = os.path.join(cur_dir, \"offset_pickles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in and arrange the data.\n",
    "sg_repeated_colnames = [\"SG 1 (V) (normalized) (compensated)\",\n",
    "                        \"SG 2 (V) (normalized) (compensated)\",\n",
    "                        \"SG 4 (V) (normalized) (compensated)\",\n",
    "                        \"SG 5 (V) (normalized) (compensated)\",\n",
    "                        \"SG 6 (V) (normalized) (compensated)\",\n",
    "                        \"SG TE (V) (normalized)\",\n",
    "                        \"SG LE (V) (normalized)\"]\n",
    "\n",
    "# Read in the offsets in IMGenie data. We'll use these values to crop out any extra lingering lines.\n",
    "with open(os.path.join(eds_rtdstr_offsets_dir,'eds_rtdstr_offsets_dynamic1.pkl'), 'rb') as f:\n",
    "  eds_rtdstr_offsets = pickle.load(f)\n",
    "with open(os.path.join(eds_rtdstr_offsets_dir,'rtdstr_offsets_trailing_dynamic1.pkl'), 'rb') as f:\n",
    "  rtdstr_offsets_trailing = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Cleaning and repetition for all the data\n",
    "###\n",
    "\n",
    "sg_dfs = list()\n",
    "\n",
    "for i in range(len(test_folders)):\n",
    "  ###\n",
    "  #First modify them according to the edits we made to the overall files (mainly crops due to different start times of EDS & UAV)\n",
    "  ###\n",
    "  data_dir = os.path.join(main_dir, test_dates[i]+\"_Tests\", \"testdata\", test_folders[i])\n",
    "  \n",
    "  #Read the RTDSTR file\n",
    "  rtdstr_filename = rtdstr_filenames[i]\n",
    "  rtdstr_file_csv = os.path.join(data_dir, rtdstr_filename)\n",
    "  rtdstr_file_df = pd.read_csv(rtdstr_file_csv+\".csv\", header=0)\n",
    "  rtdstr_file_df_edited = rtdstr_file_df\n",
    "\n",
    "  #Insert proper time object\n",
    "  month = int (test_dates[i][0:2])\n",
    "  day = int (test_dates[i][3:5])\n",
    "  year = int (test_dates[i][6:])\n",
    "  rtdstr_times = rtdstr_file_df[\"Date/Time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d_%H-%M-%S-%f\"))\n",
    "  rtdstr_times = rtdstr_times.apply(lambda x: datetime.datetime(year, month, day, x.hour, x.minute, x.second, x.microsecond)) #We do this because we haven't recorded date in the original data.\n",
    "  if \"rtdstr_DateTime Obj\" not in rtdstr_file_df_edited.columns:\n",
    "    rtdstr_file_df_edited.insert(1, \"rtdstr_DateTime Obj\", rtdstr_times)\n",
    "  else:\n",
    "    rtdstr_file_df_edited[\"rtdstr_DateTime Obj\"] = rtdstr_file_df_edited[\"rtdstr_DateTime Obj\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    \n",
    "  if eds_rtdstr_offsets[i][0] != \"rtdstr\": #if vantage[i] is not rtdstr (meaning that rtdstr started earlier, so some of it was cut).\n",
    "    #Determine how many seconds of IMGenie data we cut when aligning the data and crop out that many lines from PZT data.\n",
    "    rtdstr_file_df_edited = rtdstr_file_df_edited.iloc[eds_rtdstr_offsets[i][1]:]\n",
    "  \n",
    "  if rtdstr_offsets_trailing[i] != -1: #if vantage[i] is not rtdstr (meaning that rtdstr ended later, so some of it was cut).\n",
    "    rtdstr_file_df_edited = rtdstr_file_df_edited.iloc[:rtdstr_offsets_trailing[i]+1]\n",
    "    \n",
    "  sg_dfs.append(rtdstr_file_df_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent is complete.\n",
      "1.400073923903182 percent is complete.\n",
      "2.800147847806364 percent is complete.\n",
      "4.200221771709546 percent is complete.\n",
      "5.600295695612728 percent is complete.\n",
      "7.00036961951591 percent is complete.\n",
      "8.400443543419092 percent is complete.\n",
      "9.800517467322274 percent is complete.\n",
      "11.200591391225457 percent is complete.\n",
      "12.600665315128639 percent is complete.\n",
      "14.00073923903182 percent is complete.\n",
      "15.400813162935004 percent is complete.\n",
      "16.800887086838184 percent is complete.\n",
      "18.200961010741366 percent is complete.\n",
      "19.60103493464455 percent is complete.\n",
      "21.00110885854773 percent is complete.\n",
      "22.401182782450913 percent is complete.\n",
      "23.801256706354096 percent is complete.\n",
      "25.201330630257278 percent is complete.\n",
      "26.601404554160457 percent is complete.\n",
      "28.00147847806364 percent is complete.\n",
      "29.401552401966825 percent is complete.\n",
      "30.801626325870007 percent is complete.\n",
      "32.20170024977319 percent is complete.\n",
      "33.60177417367637 percent is complete.\n",
      "35.001848097579554 percent is complete.\n",
      "36.40192202148273 percent is complete.\n",
      "37.80199594538592 percent is complete.\n",
      "39.2020698692891 percent is complete.\n",
      "40.60214379319228 percent is complete.\n",
      "42.00221771709546 percent is complete.\n",
      "43.40229164099865 percent is complete.\n",
      "44.80236556490183 percent is complete.\n",
      "46.202439488805005 percent is complete.\n",
      "47.60251341270819 percent is complete.\n",
      "49.00258733661138 percent is complete.\n",
      "50.402661260514556 percent is complete.\n",
      "51.802735184417735 percent is complete.\n",
      "53.20280910832091 percent is complete.\n",
      "54.602883032224106 percent is complete.\n",
      "56.00295695612728 percent is complete.\n",
      "57.40303088003047 percent is complete.\n",
      "58.80310480393365 percent is complete.\n",
      "60.20317872783683 percent is complete.\n",
      "61.603252651740014 percent is complete.\n",
      "63.00332657564319 percent is complete.\n",
      "64.40340049954638 percent is complete.\n",
      "65.80347442344956 percent is complete.\n",
      "67.20354834735274 percent is complete.\n",
      "68.60362227125593 percent is complete.\n",
      "70.00369619515911 percent is complete.\n",
      "71.40377011906229 percent is complete.\n",
      "72.80384404296547 percent is complete.\n",
      "74.20391796686864 percent is complete.\n",
      "75.60399189077184 percent is complete.\n",
      "77.00406581467502 percent is complete.\n",
      "78.4041397385782 percent is complete.\n",
      "79.80421366248139 percent is complete.\n",
      "81.20428758638457 percent is complete.\n",
      "82.60436151028773 percent is complete.\n",
      "84.00443543419092 percent is complete.\n",
      "85.4045093580941 percent is complete.\n",
      "86.8045832819973 percent is complete.\n",
      "88.20465720590047 percent is complete.\n",
      "89.60473112980365 percent is complete.\n",
      "91.00480505370683 percent is complete.\n",
      "92.40487897761001 percent is complete.\n",
      "93.80495290151319 percent is complete.\n",
      "95.20502682541638 percent is complete.\n",
      "96.60510074931956 percent is complete.\n",
      "98.00517467322275 percent is complete.\n",
      "99.40524859712593 percent is complete.\n"
     ]
    }
   ],
   "source": [
    "### BEWARE: BE CAREFUL NOT TO RUN THIS BLOCK MORE THAN ONCE TIMES AS coarse_times IS MODIFIED INSIDE \n",
    "### AND RUNNING MULTIPLE TIMES WOULD GENERATE VERY INCONSISTENT DATA. \n",
    "### ALWAYS RUN THE BLOCK ABOVE RIGHT BEFORE RUNNING THIS\n",
    "\n",
    "###\n",
    "#Then add repeated lines to the SG/RTD data\n",
    "###\n",
    "def construct_repeated_sgs(fine_times, coarse_times, SG_vals):\n",
    "  #fine_times: shape(# lines in consolidated pzt)\n",
    "  #coarse_times: shape(# lines in consolidated rtd/sg)\n",
    "  #SG_vals: shape(coarse_times.shape[0], 7)\n",
    "\n",
    "  SG_vals_interp = np.zeros((fine_times.shape[0], SG_vals.shape[1]))\n",
    "  prev_time = coarse_times[0]\n",
    "  next_time_ix = 5 #This is 5 here to avoid any repeated lines at the beginning.\n",
    "  next_time = coarse_times[next_time_ix]\n",
    "  coarse_delta = next_time-prev_time\n",
    "  \n",
    "  for line_ix in range(len(fine_times)-10000): #We're ignoring the last 1 second to avoid index errors.\n",
    "    fine_time = fine_times[line_ix]\n",
    "    if line_ix % 1000000 == 0:\n",
    "      print (f\"{float(line_ix/len(fine_times)*100)} percent is complete.\")\n",
    "    if next_time - fine_time < np.timedelta64(100,\"us\"):\n",
    "      prev_time = next_time\n",
    "      next_time_ix += 1\n",
    "      time_ixs_rep_cnt = 1\n",
    "      while coarse_times[next_time_ix] == coarse_times[next_time_ix+time_ixs_rep_cnt]:\n",
    "        time_ixs_rep_cnt += 1\n",
    "      if time_ixs_rep_cnt != 1:\n",
    "        prior_time = coarse_times[next_time_ix-1]\n",
    "        next_time = coarse_times[next_time_ix+time_ixs_rep_cnt]\n",
    "        time_delta = next_time - prior_time\n",
    "        time_increment = time_delta/(time_ixs_rep_cnt+1)\n",
    "        for i in range(time_ixs_rep_cnt):\n",
    "          coarse_times[next_time_ix+i] = prior_time + time_increment*(i+1)\n",
    "      next_time = coarse_times[next_time_ix]\n",
    "      coarse_delta = next_time-prev_time\n",
    "\n",
    "    t1 = fine_time - prev_time\n",
    "    time_ratio = t1/coarse_delta\n",
    "    SG_vals_interp[line_ix] = SG_vals[next_time_ix-1]*(1-time_ratio) + SG_vals[next_time_ix]*time_ratio\n",
    "  return SG_vals_interp\n",
    "\n",
    "repeated_sg_data = list()   \n",
    "for sg_df in sg_dfs:\n",
    "  pzt_times_reconstruct = np.arange(sg_df['rtdstr_DateTime Obj'].iloc[0], sg_df['rtdstr_DateTime Obj'].iloc[-1], datetime.timedelta(0,1/10000))\n",
    "  SG_vals_interp = construct_repeated_sgs(pzt_times_reconstruct, sg_df['rtdstr_DateTime Obj'].to_numpy(), sg_df[sg_repeated_colnames].to_numpy())\n",
    "  repeated_sg_data.append({\"pzt_times_reconstruct\":pzt_times_reconstruct, \"SG_vals_interp\":SG_vals_interp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_save_dir = os.path.join(main_dir, \"ConsolidatedData\", \"Dynamic1_Jan2023\")\n",
    "repeated_sg_dfs = list()\n",
    "\n",
    "for ix, repeated_sg_dat in enumerate(repeated_sg_data):\n",
    "  repeated_sg_df = pd.DataFrame(repeated_sg_dat[\"SG_vals_interp\"], columns=sg_repeated_colnames)\n",
    "  repeated_sg_df[\"repeated_DateTime Obj\"] = repeated_sg_dat[\"pzt_times_reconstruct\"]\n",
    "  repeated_sg_df.to_pickle(os.path.join(df_save_dir,f'consolidated_repeated_sg_{ix}.pkl'))\n",
    "  repeated_sg_dfs.append(repeated_sg_df)\n",
    "\n",
    "concated_all_sg_df = pd.concat((repeated_sg_df for repeated_sg_df in repeated_sg_dfs), ignore_index=True)\n",
    "concated_all_sg_df.to_pickle(os.path.join(df_save_dir,f'consolidated_repeated_sg_all.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('python38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cae21afe383f91ea82811178060e59cc3e5895f013d1e4afce35f65192471b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
