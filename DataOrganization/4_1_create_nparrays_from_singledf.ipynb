{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(os.path.dirname(cur_dir))\n",
    "data_dir = os.path.join(main_dir, \"ConsolidatedData\", \"Dynamic1_Jan2023\")\n",
    "\n",
    "all_df_dataname = \"videotimed_labeled_consolidated_all.pkl\"\n",
    "pzt_dataname = \"consolidated_pzt_renorm_all.pkl\"\n",
    "sg_dataname = \"consolidated_repeated_sg_all.pkl\"\n",
    "\n",
    "all_df_pkl = os.path.join(data_dir, all_df_dataname)\n",
    "pzt_df_pkl = os.path.join(data_dir, pzt_dataname)\n",
    "sg_df_pkl = os.path.join(data_dir, sg_dataname)\n",
    "\n",
    "with open(all_df_pkl, 'rb') as f:\n",
    "  all_df = pickle.load(f)\n",
    "with open(pzt_df_pkl, 'rb') as f:\n",
    "  pzt_df = pickle.load(f)\n",
    "with open(sg_df_pkl, 'rb') as f:\n",
    "  sg_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736396, 119)\n",
      "(71377800, 7)\n",
      "(71424800, 8)\n"
     ]
    }
   ],
   "source": [
    "print (all_df.shape)\n",
    "print (pzt_df.shape)\n",
    "print (sg_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pzt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrange data function for training tests\n",
    "def arrange_data_training(available_airspeeds, available_aoas, available_sensors, all_df, pzt_df, sg_df):\n",
    "  # Get the total data parameters\n",
    "  #Comment1\n",
    "  print (\"Starting to record the timings and datapoints of tests.\")\n",
    "  sg_df_start_times = list()\n",
    "  shortest_test_time = datetime.timedelta(days=1)\n",
    "\n",
    "  for airspeed in available_airspeeds:\n",
    "    for aoa in available_aoas:\n",
    "      state_label = f\"{airspeed}m/s_{aoa}deg\"\n",
    "      start_t = all_df.loc[all_df[\"Label\"] == state_label][\"rtdstr_DateTime Obj\"].iloc[0]\n",
    "      end_t = all_df.loc[all_df[\"Label\"] == state_label][\"rtdstr_DateTime Obj\"].iloc[-1]\n",
    "      \n",
    "      if end_t - start_t < shortest_test_time:\n",
    "        shortest_test_time = end_t - start_t\n",
    "      sg_df_start_times.append(start_t)\n",
    "\n",
    "  test_lines = int (shortest_test_time.total_seconds() * 10000)\n",
    "  print (\"Recorded: \\n (i): start time of each training experiment \\n (ii): Number of lines in each test.\")\n",
    "  print ()\n",
    "\n",
    "  #Comment2\n",
    "  print (\"Starting to record the start and end indices of each test.\")\n",
    "  sg_df_ix = 0\n",
    "  dense_data_start_ixs = list()\n",
    "  start_rows = sg_df[sg_df[\"repeated_DateTime Obj\"].isin(sg_df_start_times)]\n",
    "  dense_data_start_ixs = start_rows.index.to_list()\n",
    "  print (\"Recorded start and end indices of each test.\")\n",
    "  print ()\n",
    "\n",
    "  #Comment3\n",
    "  state_count = len(available_airspeeds) * len(available_aoas)\n",
    "  sensor_count = len(available_sensors)\n",
    "  all_examples = np.zeros((state_count, sensor_count, test_lines))\n",
    "  all_state = np.empty(state_count, dtype=\"S20\")\n",
    "  \n",
    "  #Comment4\n",
    "  pzt_channels = [sensor_id for sensor_id in available_sensors if \"PZT\" in sensor_id]\n",
    "  sg_channels = [sensor_id + \" (V) (normalized) (compensated)\" for sensor_id in available_sensors if \"SG\" in sensor_id]\n",
    "  \n",
    "  #Comment5\n",
    "  state_id = 0\n",
    "  for airspeed in available_airspeeds:\n",
    "    for aoa in available_aoas:\n",
    "      print (f\"Processing: {airspeed}m/s_{aoa}deg\")\n",
    "\n",
    "      df_start_ix = dense_data_start_ixs[state_id]\n",
    "      df_end_ix = df_start_ix + test_lines\n",
    "      all_examples[state_id, 0:len(pzt_channels), 0:test_lines] = pzt_df.iloc[df_start_ix:df_end_ix].T\n",
    "      all_examples[state_id, len(pzt_channels):, 0:test_lines] = sg_df.loc[df_start_ix:df_end_ix-1, sg_df.columns != \"repeated_DateTime Obj\"].T\n",
    "      all_state[state_id] = f\"{airspeed}m/s_{aoa}deg\"\n",
    "\n",
    "      state_id += 1\n",
    "\n",
    "  print (\"Finished processing all data.\")\n",
    "  return (all_examples, all_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data function call for training tests\n",
    "available_airspeeds = [7, 8.3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "available_aoas = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "available_sensors = ['PZT 1', 'PZT 2', 'PZT 3', 'PZT 4', 'PZT 5', 'PZT 6', 'PZT 7', \n",
    "                    'SG 1', 'SG 2', 'SG 4', 'SG 5', 'SG 6', 'SG LE', 'SG TE']\n",
    "(allExamples, allState) = arrange_data_training (available_airspeeds, available_aoas, available_sensors, all_df, pzt_df, sg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dynamic only\n",
    "# Cut the beginning of the data to matvh the slicing in the eds data\n",
    "offsets_savedir = os.path.join(cur_dir, 'offset_pickles')\n",
    "with open(os.path.join(offsets_savedir,'starttime_dynamic1.pkl'), 'rb') as f:\n",
    "  eds_starttime = pickle.load(f)\n",
    "\n",
    "eds_rtdstr_deltas = np.abs(eds_starttime - sg_df[\"repeated_DateTime Obj\"])\n",
    "rtdstr_offset = eds_rtdstr_deltas.argmin()\n",
    "\n",
    "sg_df = sg_df[rtdstr_offset:]\n",
    "pzt_df = pzt_df[rtdstr_offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrange data function for dynamic tests\n",
    "def arrange_data_dynamic(available_sensors, all_df, pzt_df, sg_df):\n",
    "  dynamic_examples = dict()\n",
    "\n",
    "  #First get the number of datapoints in each run we'll be dealing with\n",
    "  run_all_datetimes = all_df[\"Date/Time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d_%H-%M-%S-%f\"))\n",
    "  run_rtdstr_datetimes = sg_df[\"repeated_DateTime Obj\"]\n",
    "  \n",
    "  unique_runs = all_df[\"Run Number\"].unique()\n",
    "  run_start_row_eds = 0\n",
    "  run_start_row_rtdstr = 0\n",
    "\n",
    "  pzt_channels = [sensor_id for sensor_id in available_sensors if \"PZT\" in sensor_id]\n",
    "  sg_channels = [sensor_id + \" (V) (normalized) (compensated)\" for sensor_id in available_sensors if \"SG\" in sensor_id]\n",
    "\n",
    "  for run_number in unique_runs:\n",
    "    print (f\"Processing run: {run_number}\")\n",
    "\n",
    "    run_start_time = run_all_datetimes.iloc[run_start_row_eds]\n",
    "    run_row_cnt_eds = all_df[\"Run Number\"].value_counts()[run_number]\n",
    "    if run_start_row_eds + run_row_cnt_eds < run_all_datetimes.shape[0]:\n",
    "      run_end_time = run_all_datetimes.iloc[run_start_row_eds + run_row_cnt_eds]\n",
    "      run_end_row_rtdstr = np.abs(run_rtdstr_datetimes - run_end_time).argmin()\n",
    "    else:\n",
    "      run_end_time = run_all_datetimes.iloc[-1]\n",
    "      run_end_row_rtdstr = pzt_df.shape[0] #Using PZT DF here to guarantee that we're not exceeding beyond the difference between SG and PZT data due to missing PZT samples\n",
    "    \n",
    "\n",
    "    sensor_count = len(available_sensors)\n",
    "    examples = np.zeros((sensor_count, run_end_row_rtdstr-run_start_row_rtdstr))\n",
    "\n",
    "    examples[0:len(pzt_channels), :] = pzt_df.iloc[run_start_row_rtdstr:run_end_row_rtdstr].T\n",
    "    examples[len(pzt_channels):, :] = sg_df.iloc[run_start_row_rtdstr:run_end_row_rtdstr, sg_df.columns != \"repeated_DateTime Obj\"].T\n",
    "\n",
    "    run_start_row_eds += run_row_cnt_eds\n",
    "    run_start_row_rtdstr = run_end_row_rtdstr\n",
    "\n",
    "    dynamic_examples[run_number] = examples\n",
    "  \n",
    "  return dynamic_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run: 15\n",
      "Processing run: 16\n",
      "Processing run: 17\n",
      "Processing run: 18\n",
      "Processing run: 19\n",
      "Processing run: 20\n",
      "Processing run: 21\n"
     ]
    }
   ],
   "source": [
    "#Data function call for dynamic tests\n",
    "available_sensors = ['PZT 1', 'PZT 2', 'PZT 3', 'PZT 4', 'PZT 5', 'PZT 6', 'PZT 7', \n",
    "                    'SG 1', 'SG 2', 'SG 4', 'SG 5', 'SG 6', 'SG LE', 'SG TE']\n",
    "dynamic_examples = arrange_data_dynamic (available_sensors, all_df, pzt_df, sg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.path.join(main_dir, \"KerasML\", \"Dynamic1_Jan2023\", \"data\")\n",
    "\n",
    "for run_number, example in dynamic_examples.items():\n",
    "  np.save(os.path.join(output_folder, f\"dynamic1_run{run_number}\"), example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06f4a8e307dcff81eaf561fea6e70db68b8d250e757d0576c99054ce8b3a0f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
